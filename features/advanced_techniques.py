"""
åˆ›æ„æ¿€å‘æŠ€æœ¯æ¨¡å—
åŒ…å«å¤šç§æ€ç»´æ¿€åŠ±æŠ€æœ¯ï¼šSCAMPERã€éšæœºåˆºæ¿€ã€å…­é¡¶æ€è€ƒå¸½ã€é€†å‘æ€ç»´
"""
import random
from typing import List, Dict, Any

class CreativityTechniques:
    """åˆ›æ„æ¿€å‘æŠ€æœ¯"""
    
    def __init__(self, llm_client):
        self.llm_client = llm_client
        
    def apply_scamper(self, topic: str, context: str, agent_role: str) -> str:
        """SCAMPERæ–¹æ³•ï¼šæ›¿ä»£ã€ç»„åˆã€è°ƒæ•´ã€ä¿®æ”¹ã€ç”¨é€”è½¬æ¢ã€æ¶ˆé™¤ã€é‡æ’"""
        scamper_prompts = {
            "S": "æ›¿ä»£(Substitute): æœ‰ä»€ä¹ˆå¯ä»¥è¢«æ›¿ä»£ï¼Ÿææ–™ã€æµç¨‹ã€éƒ¨ä»¶ï¼Ÿ",
            "C": "ç»„åˆ(Combine): èƒ½å¦å°†å¤šä¸ªåŠŸèƒ½ã€æƒ³æ³•æˆ–äº§å“ç»„åˆï¼Ÿ",
            "A": "è°ƒæ•´(Adapt): èƒ½å¦å€Ÿé‰´å…¶ä»–é¢†åŸŸçš„æˆåŠŸç»éªŒï¼Ÿ",
            "M": "ä¿®æ”¹(Modify): èƒ½å¦æ”¹å˜å¤§å°ã€å½¢çŠ¶ã€é¢œè‰²æˆ–å…¶ä»–å±æ€§ï¼Ÿ",
            "P": "ç”¨é€”è½¬æ¢(Put to other uses): èƒ½å¦æ‰¾åˆ°æ–°çš„åº”ç”¨åœºæ™¯ï¼Ÿ",
            "E": "æ¶ˆé™¤(Eliminate): èƒ½å¦ç®€åŒ–æˆ–å»é™¤æŸäº›éƒ¨åˆ†ï¼Ÿ",
            "R": "é‡æ’(Rearrange): èƒ½å¦æ”¹å˜é¡ºåºã€å¸ƒå±€æˆ–ç»“æ„ï¼Ÿ"
        }
        
        # éšæœºé€‰æ‹©2-3ä¸ªSCAMPERç»´åº¦
        selected = random.sample(list(scamper_prompts.items()), k=random.randint(2, 3))
        dimensions = "\n".join([f"- {v}" for k, v in selected])
        
        prompt = f"""è¯·è¿ç”¨SCAMPERåˆ›æ„æ–¹æ³•ï¼Œé’ˆå¯¹ä¸»é¢˜è¿›è¡Œåˆ›æ–°æ€è€ƒã€‚

ã€ä¸»é¢˜ã€‘{topic}
ã€èƒŒæ™¯ã€‘{context[:500]}
ã€ä½ çš„è§’è‰²ã€‘{agent_role}

ã€SCAMPERç»´åº¦ã€‘
{dimensions}

è¯·ä»ä»¥ä¸Šç»´åº¦å‡ºå‘ï¼Œç»“åˆä½ çš„ä¸“ä¸šèƒŒæ™¯ï¼Œæå‡ºåˆ›æ–°æƒ³æ³•ï¼ˆ150å­—ä»¥å†…ï¼‰ï¼š"""
        
        return self.llm_client.get_completion(
            system_prompt="ä½ æ˜¯åˆ›æ–°æ€ç»´ä¸“å®¶ï¼Œæ“…é•¿è¿ç”¨SCAMPERæ–¹æ³•æ¿€å‘åˆ›æ„ã€‚",
            user_prompt=prompt,
            model="gemini-3-pro-preview"
        )
    
    def apply_random_stimulus(self, topic: str, context: str, agent_role: str) -> str:
        """éšæœºåˆºæ¿€æ³•ï¼šç”¨éšæœºè¯æ±‡/æ¦‚å¿µæ¿€å‘è”æƒ³"""
        random_stimuli = [
            "æ°´æµ", "èœ‚å·¢", "é•œå­", "äº‘æœµ", "æ ‘æ ¹", "è´è¶æ•ˆåº”", "æ²™æ¼", "å›å£°",
            "æŒ‡çº¹", "è’²å…¬è‹±", "ç£é“", "å¿ƒè·³", "èºæ—‹", "å…‰å½±", "ç§å­", "æ¡¥æ¢",
            "è–„è†œ", "è„‰å†²", "æ°”æ³¡", "ç»‡ç½‘", "æ¸©åº¦è®¡", "é€é•œ", "é’Ÿæ‘†", "è¿·å®«"
        ]
        stimulus = random.choice(random_stimuli)
        
        prompt = f"""è¯·è¿ç”¨"éšæœºåˆºæ¿€æ³•"è¿›è¡Œåˆ›æ„è”æƒ³ã€‚

ã€ä¸»é¢˜ã€‘{topic}
ã€éšæœºåˆºæ¿€è¯ã€‘ğŸ² {stimulus}
ã€ä½ çš„è§’è‰²ã€‘{agent_role}

è¯·æ€è€ƒï¼š"{stimulus}"è¿™ä¸ªæ¦‚å¿µ/æ„è±¡ï¼Œèƒ½ç»™æˆ‘ä»¬çš„ä¸»é¢˜å¸¦æ¥ä»€ä¹ˆå¯å‘ï¼Ÿ
å°è¯•å»ºç«‹è”ç³»ï¼Œæå‡ºåˆ›æ–°æƒ³æ³•ï¼ˆ100å­—ä»¥å†…ï¼‰ï¼š"""

        return self.llm_client.get_completion(
            system_prompt="ä½ æ˜¯åˆ›æ„è”æƒ³ä¸“å®¶ï¼Œæ“…é•¿ä»çœ‹ä¼¼æ— å…³çš„äº‹ç‰©ä¸­å‘ç°åˆ›æ–°çµæ„Ÿã€‚",
            user_prompt=prompt,
            model="gemini-3-pro-preview"
        )
    
    def apply_six_hats(self, topic: str, context: str, agent_role: str) -> str:
        """å…­é¡¶æ€è€ƒå¸½ï¼šä»ä¸åŒæ€ç»´è§’åº¦åˆ†æ"""
        hats = {
            "ç™½å¸½": ("ğŸ“Š", "å®¢è§‚äº‹å®", "å…³æ³¨æ•°æ®ã€ä¿¡æ¯å’Œäº‹å®ï¼Œä¸å¸¦æƒ…ç»ªåœ°åˆ†æ"),
            "çº¢å¸½": ("â¤ï¸", "æƒ…æ„Ÿç›´è§‰", "å…³æ³¨æƒ…ç»ªã€æ„Ÿå—å’Œç›´è§‰ï¼Œä¸éœ€è¦è§£é‡Šç†ç”±"),
            "é»‘å¸½": ("âš«", "è°¨æ…æ‰¹åˆ¤", "å…³æ³¨é£é™©ã€é—®é¢˜å’Œéšœç¢ï¼Œæ‰¹åˆ¤æ€§æ€è€ƒ"),
            "é»„å¸½": ("ğŸ’›", "ä¹è§‚ç§¯æ", "å…³æ³¨ä»·å€¼ã€å¥½å¤„å’Œæœºä¼šï¼Œç§¯ææ­£é¢æ€è€ƒ"),
            "ç»¿å¸½": ("ğŸ’š", "åˆ›æ„åˆ›æ–°", "å…³æ³¨æ–°æƒ³æ³•ã€æ›¿ä»£æ–¹æ¡ˆå’Œåˆ›é€ æ€§æ€ç»´"),
            "è“å¸½": ("ğŸ’™", "è¿‡ç¨‹æ§åˆ¶", "å…³æ³¨æ€ç»´è¿‡ç¨‹ã€æ€»ç»“å’Œä¸‹ä¸€æ­¥è¡ŒåŠ¨")
        }
        
        # æ ¹æ®agentè§’è‰²é€‰æ‹©åˆé€‚çš„å¸½å­
        hat_name, (emoji, focus, desc) = random.choice(list(hats.items()))
        
        prompt = f"""è¯·æˆ´ä¸Š"{hat_name}"ï¼ˆ{focus}ï¼‰è¿›è¡Œæ€è€ƒã€‚

ã€ä¸»é¢˜ã€‘{topic}
ã€{hat_name}æ€ç»´ã€‘{emoji} {desc}
ã€ä½ çš„è§’è‰²ã€‘{agent_role}

è¯·ä»{hat_name}çš„è§’åº¦ï¼Œé’ˆå¯¹ä¸»é¢˜å‘è¡¨è§‚ç‚¹ï¼ˆ100å­—ä»¥å†…ï¼‰ï¼š"""

        return self.llm_client.get_completion(
            system_prompt=f"ä½ æ­£åœ¨è¿ç”¨å…­é¡¶æ€è€ƒå¸½æ–¹æ³•ï¼Œç°åœ¨æˆ´ç€{hat_name}ï¼Œä¸“æ³¨äº{focus}ã€‚",
            user_prompt=prompt,
            model="gemini-3-pro-preview"
        )
    
    def apply_reverse_thinking(self, topic: str, context: str, agent_role: str) -> str:
        """é€†å‘æ€ç»´ï¼šä»åé¢æ€è€ƒé—®é¢˜"""
        prompt = f"""è¯·è¿ç”¨"é€†å‘æ€ç»´"æ–¹æ³•è¿›è¡Œåˆ›æ–°æ€è€ƒã€‚

ã€ä¸»é¢˜ã€‘{topic}
ã€èƒŒæ™¯ã€‘{context[:300]}
ã€ä½ çš„è§’è‰²ã€‘{agent_role}

ã€é€†å‘æ€ç»´æ­¥éª¤ã€‘
1. å¦‚æœæˆ‘ä»¬æƒ³è®©è¿™ä¸ªä¸»é¢˜å½»åº•å¤±è´¥ï¼Œåº”è¯¥æ€ä¹ˆåšï¼Ÿ
2. æŠŠè¿™äº›"å¤±è´¥æ–¹æ³•"åè¿‡æ¥ï¼Œèƒ½å¾—åˆ°ä»€ä¹ˆå¯å‘ï¼Ÿ
3. æå‡ºä½ çš„åˆ›æ–°å»ºè®®

è¯·è¿ç”¨é€†å‘æ€ç»´ï¼Œæå‡ºåˆ›æ–°æƒ³æ³•ï¼ˆ150å­—ä»¥å†…ï¼‰ï¼š"""

        return self.llm_client.get_completion(
            system_prompt="ä½ æ˜¯é€†å‘æ€ç»´ä¸“å®¶ï¼Œæ“…é•¿ä»é—®é¢˜çš„åé¢æ‰¾åˆ°åˆ›æ–°çªç ´å£ã€‚",
            user_prompt=prompt,
            model="gemini-3-pro-preview"
        )
    
    def stimulate_creativity(self, topic: str, context: str, agent_role: str, technique: str = None) -> Dict[str, str]:
        """åº”ç”¨åˆ›æ„æ¿€å‘æŠ€æœ¯"""
        techniques = {
            "scamper": ("SCAMPERæ–¹æ³•", self.apply_scamper),
            "random_input": ("éšæœºåˆºæ¿€æ³•", self.apply_random_stimulus),
            "six_thinking_hats": ("å…­é¡¶æ€è€ƒå¸½", self.apply_six_hats),
            "reverse_thinking": ("é€†å‘æ€ç»´", self.apply_reverse_thinking)
        }
        
        if technique is None:
            technique = random.choice(list(techniques.keys()))
        
        name, func = techniques[technique]
        result = func(topic, context, agent_role)
        
        return {
            "technique": technique,
            "technique_name": name,
            "result": result
        }


class IdeaEvolution:
    """æƒ³æ³•è¿›åŒ–ç®—æ³•ï¼šé€šè¿‡å˜å¼‚ã€äº¤å‰ã€é€‰æ‹©æ¥ä¼˜åŒ–æƒ³æ³•"""
    
    def __init__(self, llm_client):
        self.llm_client = llm_client
    
    def mutate_idea(self, idea: str, mutation_type: str, topic: str) -> str:
        """å¯¹æƒ³æ³•è¿›è¡Œå˜å¼‚"""
        mutations = {
            "amplify": "æ”¾å¤§è¿™ä¸ªæƒ³æ³•çš„æ ¸å¿ƒä¼˜åŠ¿ï¼Œè®©å®ƒæ›´åŠ çªå‡º",
            "minimize": "ç®€åŒ–è¿™ä¸ªæƒ³æ³•ï¼Œæ‰¾åˆ°æœ€å°å¯è¡Œç‰ˆæœ¬",
            "combine": "å°†è¿™ä¸ªæƒ³æ³•ä¸å¦ä¸€ä¸ªé¢†åŸŸçš„æˆåŠŸæ¡ˆä¾‹ç»“åˆ",
            "reverse": "åè½¬è¿™ä¸ªæƒ³æ³•çš„æŸä¸ªå…³é”®å‡è®¾",
            "extreme": "æŠŠè¿™ä¸ªæƒ³æ³•æ¨å‘æç«¯ï¼Œçœ‹çœ‹ä¼šå‘ç”Ÿä»€ä¹ˆ"
        }
        
        mutation_desc = mutations.get(mutation_type, mutations["amplify"])
        
        prompt = f"""è¯·å¯¹ä»¥ä¸‹æƒ³æ³•è¿›è¡Œ"å˜å¼‚"ä¼˜åŒ–ã€‚

ã€åŸå§‹æƒ³æ³•ã€‘{idea}
ã€è®¨è®ºä¸»é¢˜ã€‘{topic}
ã€å˜å¼‚æ–¹å‘ã€‘{mutation_desc}

è¯·ç”Ÿæˆå˜å¼‚åçš„æ–°æƒ³æ³•ï¼ˆ100å­—ä»¥å†…ï¼‰ï¼š"""

        return self.llm_client.get_completion(
            system_prompt="ä½ æ˜¯åˆ›æ„è¿›åŒ–ä¸“å®¶ï¼Œæ“…é•¿é€šè¿‡å˜å¼‚ä¼˜åŒ–æƒ³æ³•ã€‚",
            user_prompt=prompt,
            model="gemini-3-pro-preview"
        )
    
    def crossover_ideas(self, idea1: str, idea2: str, topic: str) -> str:
        """äº¤å‰ä¸¤ä¸ªæƒ³æ³•ï¼Œäº§ç”Ÿæ–°æƒ³æ³•"""
        prompt = f"""è¯·å°†ä»¥ä¸‹ä¸¤ä¸ªæƒ³æ³•è¿›è¡Œ"äº¤å‰"ï¼Œäº§ç”Ÿä¸€ä¸ªèåˆä¸¤è€…ä¼˜åŠ¿çš„æ–°æƒ³æ³•ã€‚

ã€æƒ³æ³•Aã€‘{idea1}
ã€æƒ³æ³•Bã€‘{idea2}
ã€è®¨è®ºä¸»é¢˜ã€‘{topic}

è¯·ç”Ÿæˆäº¤å‰åçš„æ–°æƒ³æ³•ï¼Œç»“åˆä¸¤è€…çš„ç²¾åï¼ˆ100å­—ä»¥å†…ï¼‰ï¼š"""

        return self.llm_client.get_completion(
            system_prompt="ä½ æ˜¯åˆ›æ„èåˆä¸“å®¶ï¼Œæ“…é•¿å°†ä¸åŒæƒ³æ³•äº¤å‰äº§ç”Ÿåˆ›æ–°ã€‚",
            user_prompt=prompt,
            model="gemini-3-pro-preview"
        )
    
    def evaluate_fitness(self, idea: str, topic: str) -> Dict[str, Any]:
        """è¯„ä¼°æƒ³æ³•çš„é€‚åº”åº¦"""
        prompt = f"""è¯·è¯„ä¼°ä»¥ä¸‹åˆ›æ–°æƒ³æ³•çš„è´¨é‡ã€‚

ã€æƒ³æ³•ã€‘{idea}
ã€ä¸»é¢˜ã€‘{topic}

è¯·ä»ä»¥ä¸‹ç»´åº¦è¯„åˆ†(1-10åˆ†)å¹¶ç»™å‡ºç®€çŸ­ç†ç”±ï¼š
1. åˆ›æ–°æ€§
2. å¯è¡Œæ€§
3. ä»·å€¼æ½œåŠ›
4. é£é™©ç¨‹åº¦

è¯·ç”¨JSONæ ¼å¼å›ç­”ï¼š"""

        result = self.llm_client.get_completion(
            system_prompt="ä½ æ˜¯åˆ›æ–°è¯„ä¼°ä¸“å®¶ï¼Œå®¢è§‚è¯„ä»·æƒ³æ³•è´¨é‡ã€‚",
            user_prompt=prompt,
            model="gemini-3-pro-preview"
        )
        
        # ç®€å•è§£æï¼Œè¿”å›åŸå§‹æ–‡æœ¬
        return {"evaluation": result, "idea": idea}
    
    def evolve_ideas(self, ideas: List[str], topic: str, generations: int = 2) -> List[Dict]:
        """è¿›åŒ–ä¸€ç»„æƒ³æ³•"""
        evolved = []
        
        for gen in range(generations):
            # é€‰æ‹©ä¸¤ä¸ªæƒ³æ³•è¿›è¡Œäº¤å‰
            if len(ideas) >= 2:
                pair = random.sample(ideas, 2)
                crossed = self.crossover_ideas(pair[0], pair[1], topic)
                evolved.append({
                    "type": "crossover",
                    "parents": pair,
                    "result": crossed,
                    "generation": gen + 1
                })
            
            # éšæœºå˜å¼‚ä¸€ä¸ªæƒ³æ³•
            if ideas:
                base = random.choice(ideas)
                mutation = random.choice(["amplify", "minimize", "combine", "reverse", "extreme"])
                mutated = self.mutate_idea(base, mutation, topic)
                evolved.append({
                    "type": "mutation",
                    "base": base,
                    "mutation_type": mutation,
                    "result": mutated,
                    "generation": gen + 1
                })
        
        return evolved


class ParallelDivergence:
    """å¹³è¡Œå‘æ•£æ¨¡å¼ï¼šæ‰€æœ‰æ™ºèƒ½ä½“åŒæ—¶ç‹¬ç«‹äº§ç”Ÿæƒ³æ³•"""
    
    def __init__(self, llm_client):
        self.llm_client = llm_client
    
    def generate_parallel_ideas(self, topic: str, agents: List[Any], context: str = "") -> List[Dict]:
        """æ‰€æœ‰æ™ºèƒ½ä½“åŒæ—¶äº§ç”Ÿæƒ³æ³•"""
        all_ideas = []
        
        prompt_template = """ã€å¹³è¡Œå‘æ•£æ¨¡å¼ã€‘
è¯·ç‹¬ç«‹æ€è€ƒï¼Œä¸è¦å—å…¶ä»–äººå½±å“ï¼Œé’ˆå¯¹ä¸»é¢˜æå‡ºä½ çš„ç‹¬ç‰¹æƒ³æ³•ã€‚

ã€ä¸»é¢˜ã€‘{topic}
ã€ä½ çš„è§’è‰²ã€‘{role}
ã€ä½ çš„ä¸“é•¿ã€‘{expertise}

è¦æ±‚ï¼š
1. ä»ä½ çš„ä¸“ä¸šè§’åº¦å‡ºå‘
2. æå‡º1-2ä¸ªç‹¬ç‰¹æƒ³æ³•
3. æ¯ä¸ªæƒ³æ³•ç®€æ´æ˜äº†ï¼ˆ50å­—ä»¥å†…ï¼‰

è¯·ç›´æ¥åˆ—å‡ºä½ çš„æƒ³æ³•ï¼š"""

        for agent in agents:
            prompt = prompt_template.format(
                topic=topic,
                role=agent.role,
                expertise=agent.expertise
            )
            
            result = self.llm_client.get_completion(
                system_prompt=agent.get_system_prompt(),
                user_prompt=prompt,
                model=agent.model_name
            )
            
            all_ideas.append({
                "agent": agent.name,
                "role": agent.role,
                "ideas": result
            })
        
        return all_ideas
    
    async def generate_parallel_ideas_async(self, topic: str, agents: List[Any], context: str = "") -> List[Dict]:
        """
        çœŸæ­£çš„å¹¶è¡Œå‘æ•£ - æ‰€æœ‰ Agent åŒæ—¶ç”Ÿæˆæƒ³æ³•
        
        ä½¿ç”¨ asyncio.gather å®ç°çœŸæ­£çš„å¹¶è¡Œè°ƒç”¨ï¼Œ
        æ¯”é¡ºåºè°ƒç”¨å¿« N å€ï¼ˆN = Agent æ•°é‡ï¼‰
        
        ä½¿ç”¨æ–¹æ³•:
            ideas = await divergence.generate_parallel_ideas_async(topic, agents)
        """
        import asyncio
        
        prompt_template = """ã€å¹³è¡Œå‘æ•£æ¨¡å¼ã€‘
è¯·ç‹¬ç«‹æ€è€ƒï¼Œä¸è¦å—å…¶ä»–äººå½±å“ï¼Œé’ˆå¯¹ä¸»é¢˜æå‡ºä½ çš„ç‹¬ç‰¹æƒ³æ³•ã€‚

ã€ä¸»é¢˜ã€‘{topic}
ã€ä½ çš„è§’è‰²ã€‘{role}
ã€ä½ çš„ä¸“é•¿ã€‘{expertise}

è¦æ±‚ï¼š
1. ä»ä½ çš„ä¸“ä¸šè§’åº¦å‡ºå‘
2. æå‡º1-2ä¸ªç‹¬ç‰¹æƒ³æ³•
3. æ¯ä¸ªæƒ³æ³•ç®€æ´æ˜äº†ï¼ˆ50å­—ä»¥å†…ï¼‰

è¯·ç›´æ¥åˆ—å‡ºä½ çš„æƒ³æ³•ï¼š"""

        async def generate_for_agent(agent):
            """å•ä¸ª Agent çš„å¼‚æ­¥ç”Ÿæˆä»»åŠ¡"""
            prompt = prompt_template.format(
                topic=topic,
                role=agent.role,
                expertise=agent.expertise
            )
            
            # ä½¿ç”¨å¼‚æ­¥å®¢æˆ·ç«¯
            result = await self.llm_client.get_completion_async(
                system_prompt=agent.get_system_prompt(),
                user_prompt=prompt,
                model=agent.model_name
            )
            
            return {
                "agent": agent.name,
                "role": agent.role,
                "ideas": result
            }
        
        # æ‰€æœ‰ Agent å¹¶è¡Œæ‰§è¡Œ
        tasks = [generate_for_agent(agent) for agent in agents]
        results = await asyncio.gather(*tasks)
        
        return list(results)
    
    def deduplicate_and_cluster(self, ideas: List[Dict], topic: str) -> str:
        """å»é‡å¹¶èšç±»æƒ³æ³•"""
        ideas_text = "\n".join([f"ã€{i['agent']}ã€‘{i['ideas']}" for i in ideas])
        
        prompt = f"""è¯·æ•´ç†ä»¥ä¸‹å„ä¸“å®¶ç‹¬ç«‹æå‡ºçš„æƒ³æ³•ã€‚

{ideas_text}

è¯·ï¼š
1. è¯†åˆ«ç›¸ä¼¼/é‡å¤çš„æƒ³æ³•
2. å°†æƒ³æ³•æŒ‰ä¸»é¢˜èšç±»
3. æç‚¼å‡ºæœ€æœ‰ä»·å€¼çš„æ ¸å¿ƒæƒ³æ³•

è¾“å‡ºæ•´ç†åçš„æƒ³æ³•æ¸…å•ï¼š"""

        return self.llm_client.get_completion(
            system_prompt="ä½ æ˜¯åˆ›æ„æ•´ç†ä¸“å®¶ï¼Œæ“…é•¿ä»å¤§é‡æƒ³æ³•ä¸­æç‚¼ç²¾åã€‚",
            user_prompt=prompt,
            model="gemini-3-pro-preview"
        )


class ChainDeepening:
    """é“¾å¼æ·±åŒ–æ¨¡å¼ï¼šæƒ³æ³•åœ¨æ™ºèƒ½ä½“é—´ä¼ é€’æ·±åŒ–"""
    
    def __init__(self, llm_client):
        self.llm_client = llm_client
    
    def deepen_chain(self, seed_idea: str, agents: List[Any], topic: str) -> List[Dict]:
        """æƒ³æ³•åœ¨æ™ºèƒ½ä½“é—´ä¼ é€’æ·±åŒ–"""
        chain = []
        current_idea = seed_idea
        
        for i, agent in enumerate(agents):
            prompt = f"""ã€é“¾å¼æ·±åŒ–æ¨¡å¼ã€‘
ä½ æ˜¯æ·±åŒ–é“¾æ¡çš„ç¬¬{i+1}ç¯ã€‚è¯·åœ¨å‰ä¸€ä¸ªæƒ³æ³•çš„åŸºç¡€ä¸Šï¼Œä»ä½ çš„ä¸“ä¸šè§’åº¦è¿›è¡Œæ·±åŒ–ã€‚

ã€è®¨è®ºä¸»é¢˜ã€‘{topic}
ã€å½“å‰æƒ³æ³•ã€‘{current_idea}
ã€ä½ çš„è§’è‰²ã€‘{agent.role}
ã€ä½ çš„ä¸“é•¿ã€‘{agent.expertise}

è¯·ä»ä½ çš„ä¸“ä¸šè§’åº¦ï¼š
1. è¡¥å……æŠ€æœ¯ç»†èŠ‚æˆ–å®ç°æ–¹æ¡ˆ
2. æŒ‡å‡ºæ½œåœ¨é—®é¢˜å’Œè§£å†³æ€è·¯
3. å¢åŠ ä½ çš„ä¸“ä¸šè§†è§’

æ·±åŒ–åçš„æƒ³æ³•ï¼ˆ150å­—ä»¥å†…ï¼‰ï¼š"""

            result = self.llm_client.get_completion(
                system_prompt=agent.get_system_prompt(),
                user_prompt=prompt,
                model=agent.model_name
            )
            
            chain.append({
                "step": i + 1,
                "agent": agent.name,
                "role": agent.role,
                "input": current_idea,
                "output": result
            })
            
            current_idea = result
        
        return chain


class DebateMode:
    """è¾©è®ºæ¨¡å¼ï¼šæ­£åæ–¹è¾©è®ºè¯„ä¼°æƒ³æ³•"""
    
    def __init__(self, llm_client):
        self.llm_client = llm_client
    
    def argue_for(self, idea: str, agent: Any, topic: str) -> str:
        """æ­£æ–¹è®ºè¯"""
        prompt = f"""ã€è¾©è®ºæ¨¡å¼ - æ­£æ–¹ã€‘
ä½ éœ€è¦ä¸ºä»¥ä¸‹æƒ³æ³•è¿›è¡Œè¾©æŠ¤ï¼Œè¯´æ˜å…¶ä»·å€¼å’Œå¯è¡Œæ€§ã€‚

ã€è®¨è®ºä¸»é¢˜ã€‘{topic}
ã€å¾…è¾©æŠ¤çš„æƒ³æ³•ã€‘{idea}
ã€ä½ çš„è§’è‰²ã€‘{agent.role}

è¯·ä»ä½ çš„ä¸“ä¸šè§’åº¦ï¼Œåˆ—å‡º3ä¸ªæ”¯æŒè¿™ä¸ªæƒ³æ³•çš„è®ºç‚¹ï¼ˆ100å­—ä»¥å†…ï¼‰ï¼š"""

        return self.llm_client.get_completion(
            system_prompt=f"ä½ æ˜¯è¾©è®ºèµ›æ­£æ–¹ä»£è¡¨ï¼Œä½ çš„è§’è‰²æ˜¯{agent.role}ï¼Œéœ€è¦æœ‰ç†æœ‰æ®åœ°æ”¯æŒè¿™ä¸ªæƒ³æ³•ã€‚",
            user_prompt=prompt,
            model=agent.model_name
        )
    
    def argue_against(self, idea: str, agent: Any, topic: str) -> str:
        """åæ–¹è®ºè¯"""
        prompt = f"""ã€è¾©è®ºæ¨¡å¼ - åæ–¹ã€‘
ä½ éœ€è¦å¯¹ä»¥ä¸‹æƒ³æ³•æå‡ºè´¨ç–‘ï¼ŒæŒ‡å‡ºå…¶é—®é¢˜å’Œé£é™©ã€‚

ã€è®¨è®ºä¸»é¢˜ã€‘{topic}
ã€å¾…è´¨ç–‘çš„æƒ³æ³•ã€‘{idea}
ã€ä½ çš„è§’è‰²ã€‘{agent.role}

è¯·ä»ä½ çš„ä¸“ä¸šè§’åº¦ï¼Œåˆ—å‡º3ä¸ªè´¨ç–‘è¿™ä¸ªæƒ³æ³•çš„è®ºç‚¹ï¼ˆ100å­—ä»¥å†…ï¼‰ï¼š"""

        return self.llm_client.get_completion(
            system_prompt=f"ä½ æ˜¯è¾©è®ºèµ›åæ–¹ä»£è¡¨ï¼Œä½ çš„è§’è‰²æ˜¯{agent.role}ï¼Œéœ€è¦ç†æ€§åœ°è´¨ç–‘å’ŒæŒ‘æˆ˜è¿™ä¸ªæƒ³æ³•ã€‚",
            user_prompt=prompt,
            model=agent.model_name
        )
    
    def run_debate(self, idea: str, pro_agents: List[Any], con_agents: List[Any], topic: str) -> Dict:
        """æ‰§è¡Œè¾©è®º"""
        pro_arguments = []
        con_arguments = []
        
        # æ­£æ–¹å‘è¨€
        for agent in pro_agents:
            arg = self.argue_for(idea, agent, topic)
            pro_arguments.append({
                "agent": agent.name,
                "role": agent.role,
                "argument": arg
            })
        
        # åæ–¹å‘è¨€
        for agent in con_agents:
            arg = self.argue_against(idea, agent, topic)
            con_arguments.append({
                "agent": agent.name,
                "role": agent.role,
                "argument": arg
            })
        
        # ç»¼åˆè¾©è®ºç»“æœ
        synthesis = self.synthesize_debate(idea, pro_arguments, con_arguments, topic)
        
        return {
            "idea": idea,
            "pro_arguments": pro_arguments,
            "con_arguments": con_arguments,
            "synthesis": synthesis
        }
    
    def synthesize_debate(self, idea: str, pro: List[Dict], con: List[Dict], topic: str) -> str:
        """ç»¼åˆè¾©è®ºç»“è®º"""
        pro_text = "\n".join([f"ã€{p['agent']}ã€‘{p['argument']}" for p in pro])
        con_text = "\n".join([f"ã€{c['agent']}ã€‘{c['argument']}" for c in con])
        
        prompt = f"""è¯·ç»¼åˆä»¥ä¸‹è¾©è®ºï¼Œå¾—å‡ºå®¢è§‚ç»“è®ºã€‚

ã€ä¸»é¢˜ã€‘{topic}
ã€è®¨è®ºçš„æƒ³æ³•ã€‘{idea}

ã€æ­£æ–¹è§‚ç‚¹ã€‘
{pro_text}

ã€åæ–¹è§‚ç‚¹ã€‘
{con_text}

è¯·ç»¼åˆåŒæ–¹è§‚ç‚¹ï¼š
1. è¿™ä¸ªæƒ³æ³•çš„æ ¸å¿ƒä»·å€¼æ˜¯ä»€ä¹ˆï¼Ÿ
2. ä¸»è¦é£é™©å’ŒæŒ‘æˆ˜æ˜¯ä»€ä¹ˆï¼Ÿ
3. å¦‚ä½•æ”¹è¿›è¿™ä¸ªæƒ³æ³•ï¼Ÿ
4. æœ€ç»ˆå»ºè®®ï¼ˆæ¨è/è°¨æ…æ¨è¿›/æš‚ç¼“ï¼‰

è¯·ç»™å‡ºç»¼åˆç»“è®ºï¼ˆ200å­—ä»¥å†…ï¼‰ï¼š"""

        return self.llm_client.get_completion(
            system_prompt="ä½ æ˜¯å…¬æ­£çš„è¾©è®ºè£åˆ¤ï¼Œéœ€è¦å®¢è§‚ç»¼åˆåŒæ–¹è§‚ç‚¹å¾—å‡ºç»“è®ºã€‚",
            user_prompt=prompt,
            model="gemini-3-pro-preview"
        )
    
    async def run_debate_async(self, idea: str, pro_agents: List[Any], con_agents: List[Any], topic: str) -> Dict:
        """
        å¼‚æ­¥è¾©è®ºæ¨¡å¼ - æ­£åæ–¹å¹¶è¡Œå‘è¨€
        
        æ‰€æœ‰ Pro å’Œ Con Agent åŒæ—¶ç”Ÿæˆè®ºç‚¹ï¼Œå¤§å¹…æå‡é€Ÿåº¦
        """
        import asyncio
        
        async def argue_for_async(agent):
            prompt = f"""ã€è¾©è®ºæ¨¡å¼ - æ­£æ–¹ã€‘
ä½ éœ€è¦ä¸ºä»¥ä¸‹æƒ³æ³•è¿›è¡Œè¾©æŠ¤ï¼Œè¯´æ˜å…¶ä»·å€¼å’Œå¯è¡Œæ€§ã€‚

ã€è®¨è®ºä¸»é¢˜ã€‘{topic}
ã€å¾…è¾©æŠ¤çš„æƒ³æ³•ã€‘{idea}
ã€ä½ çš„è§’è‰²ã€‘{agent.role}

è¯·ä»ä½ çš„ä¸“ä¸šè§’åº¦ï¼Œåˆ—å‡º3ä¸ªæ”¯æŒè¿™ä¸ªæƒ³æ³•çš„è®ºç‚¹ï¼ˆ100å­—ä»¥å†…ï¼‰ï¼š"""
            
            result = await self.llm_client.get_completion_async(
                system_prompt=f"ä½ æ˜¯è¾©è®ºèµ›æ­£æ–¹ä»£è¡¨ï¼Œä½ çš„è§’è‰²æ˜¯{agent.role}ï¼Œéœ€è¦æœ‰ç†æœ‰æ®åœ°æ”¯æŒè¿™ä¸ªæƒ³æ³•ã€‚",
                user_prompt=prompt,
                model=agent.model_name
            )
            return {"agent": agent.name, "role": agent.role, "argument": result}
        
        async def argue_against_async(agent):
            prompt = f"""ã€è¾©è®ºæ¨¡å¼ - åæ–¹ã€‘
ä½ éœ€è¦å¯¹ä»¥ä¸‹æƒ³æ³•æå‡ºè´¨ç–‘ï¼ŒæŒ‡å‡ºå…¶é—®é¢˜å’Œé£é™©ã€‚

ã€è®¨è®ºä¸»é¢˜ã€‘{topic}
ã€å¾…è´¨ç–‘çš„æƒ³æ³•ã€‘{idea}
ã€ä½ çš„è§’è‰²ã€‘{agent.role}

è¯·ä»ä½ çš„ä¸“ä¸šè§’åº¦ï¼Œåˆ—å‡º3ä¸ªè´¨ç–‘è¿™ä¸ªæƒ³æ³•çš„è®ºç‚¹ï¼ˆ100å­—ä»¥å†…ï¼‰ï¼š"""
            
            result = await self.llm_client.get_completion_async(
                system_prompt=f"ä½ æ˜¯è¾©è®ºèµ›åæ–¹ä»£è¡¨ï¼Œä½ çš„è§’è‰²æ˜¯{agent.role}ï¼Œéœ€è¦ç†æ€§åœ°è´¨ç–‘å’ŒæŒ‘æˆ˜è¿™ä¸ªæƒ³æ³•ã€‚",
                user_prompt=prompt,
                model=agent.model_name
            )
            return {"agent": agent.name, "role": agent.role, "argument": result}
        
        # æ­£åæ–¹å¹¶è¡Œæ‰§è¡Œ
        pro_tasks = [argue_for_async(agent) for agent in pro_agents]
        con_tasks = [argue_against_async(agent) for agent in con_agents]
        
        all_results = await asyncio.gather(*pro_tasks, *con_tasks)
        
        pro_arguments = all_results[:len(pro_agents)]
        con_arguments = all_results[len(pro_agents):]
        
        # ç»¼åˆè¾©è®ºç»“æœ
        synthesis = self.synthesize_debate(idea, pro_arguments, con_arguments, topic)
        
        return {
            "idea": idea,
            "pro_arguments": pro_arguments,
            "con_arguments": con_arguments,
            "synthesis": synthesis
        }

