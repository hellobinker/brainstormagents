# -*- coding: utf-8 -*-
"""
Async Parallel Divergence - True parallel idea generation using asyncio
"""
import asyncio
from typing import List, Dict, Any
from utils.async_llm_client import AsyncLLMClient


class AsyncParallelDivergence:
    """
    Parallel divergence mode with true async execution.
    All agents generate ideas simultaneously using asyncio.gather().
    """
    
    def __init__(self, async_client: AsyncLLMClient):
        self.async_client = async_client
    
    async def generate_parallel_ideas_async(
        self, 
        topic: str, 
        agents: List[Any], 
        context: str = ""
    ) -> List[Dict[str, Any]]:
        """
        All agents generate ideas truly in parallel.
        
        Args:
            topic: The brainstorming topic
            agents: List of Agent objects
            context: Optional discussion context
        
        Returns:
            List of idea dicts: [{"agent": name, "role": role, "ideas": content}, ...]
        """
        # Build prompts for each agent
        prompts = []
        for agent in agents:
            prompt = f"""ã€å¹³è¡Œå‘æ•£æ¨¡å¼ã€‘ğŸ§ 

ç°åœ¨æ˜¯å¤´è„‘é£æš´çš„å¹³è¡Œå‘æ•£ç¯èŠ‚ã€‚è¯·ä»ä½ çš„ä¸“ä¸šè§’åº¦({agent.expertise})å‡ºå‘ï¼Œ
é’ˆå¯¹ä¸»é¢˜æå‡º2-3ä¸ªç‹¬ç«‹çš„åˆ›æ–°æƒ³æ³•ã€‚

ã€ä¸»é¢˜ã€‘{topic}

{"ã€å‚è€ƒä¸Šä¸‹æ–‡ã€‘" + context if context else ""}

è¯·ç›´æ¥åˆ—å‡ºä½ çš„æƒ³æ³•ï¼Œæ¯ä¸ªæƒ³æ³•ç”¨ â€¢ å¼€å¤´ï¼š"""
            
            prompts.append({
                "system_prompt": agent.get_system_prompt(),
                "user_prompt": prompt,
                "model": agent.model_name
            })
        
        # Execute all completions in parallel
        responses = await self.async_client.get_parallel_completions(prompts)
        
        # Map responses to agents
        results = []
        for agent, response in zip(agents, responses):
            results.append({
                "agent": agent.name,
                "role": agent.role,
                "ideas": response
            })
        
        return results
    
    async def generate_and_cluster_async(
        self, 
        topic: str, 
        agents: List[Any], 
        context: str = ""
    ) -> Dict[str, Any]:
        """
        Generate parallel ideas and then cluster them.
        
        Returns:
            Dict with parallel_ideas and clustered summary
        """
        # Step 1: Parallel generation
        all_ideas = await self.generate_parallel_ideas_async(topic, agents, context)
        
        # Step 2: Cluster and deduplicate
        ideas_text = "\n".join([
            f"ã€{item['agent']} ({item['role']})ã€‘\n{item['ideas']}"
            for item in all_ideas
        ])
        
        cluster_prompt = f"""è¯·å¯¹ä»¥ä¸‹å¤šä½ä¸“å®¶çš„æƒ³æ³•è¿›è¡Œæ•´ç†å’Œå½’ç±»ï¼š

{ideas_text}

è¯·ï¼š
1. è¯†åˆ«å…±åŒä¸»é¢˜å’Œç‹¬ç‰¹è§‚ç‚¹
2. åˆå¹¶ç›¸ä¼¼æƒ³æ³•
3. æŒ‰ä¸»é¢˜åˆ†ç±»æ•´ç†
4. è¾“å‡ºç»“æ„åŒ–çš„æƒ³æ³•æ¸…å•"""
        
        clustered = await self.async_client.get_completion_async(
            system_prompt="ä½ æ˜¯åˆ›æ„æ•´ç†ä¸“å®¶ï¼Œæ“…é•¿å½’çº³å’Œåˆ†ç±»æƒ³æ³•ã€‚",
            user_prompt=cluster_prompt
        )
        
        return {
            "parallel_ideas": all_ideas,
            "clustered": clustered
        }


class AsyncChainDeepening:
    """
    Chain deepening with async execution.
    Ideas flow through agents sequentially but each step is async.
    """
    
    def __init__(self, async_client: AsyncLLMClient):
        self.async_client = async_client
    
    async def deepen_chain_async(
        self, 
        seed_idea: str, 
        agents: List[Any], 
        topic: str
    ) -> List[Dict[str, Any]]:
        """
        Pass idea through agent chain, each agent deepens it.
        """
        chain = []
        current_idea = seed_idea
        
        for i, agent in enumerate(agents):
            prompt = f"""ã€é“¾å¼æ·±åŒ–æ¨¡å¼ã€‘ğŸ”—

ä½ æ˜¯é“¾å¼æ·±åŒ–çš„ç¬¬ {i+1} ç¯ã€‚è¯·åœ¨å‰ä¸€ä½ä¸“å®¶çš„æƒ³æ³•åŸºç¡€ä¸Šè¿›è¡Œæ·±åŒ–å’Œæ‰©å±•ã€‚

ã€ä¸»é¢˜ã€‘{topic}
ã€å½“å‰æƒ³æ³•ã€‘{current_idea}

è¯·ä»ä½ çš„ä¸“ä¸šè§’åº¦({agent.expertise})æ·±åŒ–è¿™ä¸ªæƒ³æ³•ï¼š
1. æŒ‡å‡ºä¼˜ç‚¹å’Œå¯è¡Œæ€§
2. è¡¥å……å…·ä½“å®ç°ç»†èŠ‚
3. æå‡ºæ”¹è¿›å»ºè®®

è¯·ç”¨100å­—ä»¥å†…å›åº”ï¼š"""
            
            response = await self.async_client.get_completion_async(
                system_prompt=agent.get_system_prompt(),
                user_prompt=prompt,
                model=agent.model_name
            )
            
            chain.append({
                "step": i + 1,
                "agent": agent.name,
                "role": agent.role,
                "input": current_idea,
                "output": response
            })
            
            current_idea = response
        
        return chain


class AsyncDebateMode:
    """
    Debate mode with parallel pro/con arguments.
    """
    
    def __init__(self, async_client: AsyncLLMClient):
        self.async_client = async_client
    
    async def run_debate_async(
        self, 
        idea: str, 
        pro_agents: List[Any], 
        con_agents: List[Any], 
        topic: str
    ) -> Dict[str, Any]:
        """
        Run debate with pro and con arguments generated in parallel.
        """
        # Prepare pro prompts
        pro_prompts = []
        for agent in pro_agents:
            pro_prompts.append({
                "system_prompt": agent.get_system_prompt(),
                "user_prompt": f"""ã€è¾©è®ºæ¨¡å¼ - æ­£æ–¹ã€‘ğŸ‘

è¯·è®ºè¯ä»¥ä¸‹æƒ³æ³•çš„ä¼˜ç‚¹å’Œå¯è¡Œæ€§ï¼š

ã€ä¸»é¢˜ã€‘{topic}
ã€å¾…è¾©è®ºæƒ³æ³•ã€‘{idea}

è¯·ä»ä½ çš„ä¸“ä¸šè§’åº¦æä¾›æœ‰åŠ›çš„æ”¯æŒè®ºç‚¹ï¼ˆ100å­—ä»¥å†…ï¼‰ï¼š""",
                "model": agent.model_name
            })
        
        # Prepare con prompts
        con_prompts = []
        for agent in con_agents:
            con_prompts.append({
                "system_prompt": agent.get_system_prompt(),
                "user_prompt": f"""ã€è¾©è®ºæ¨¡å¼ - åæ–¹ã€‘ğŸ‘

è¯·æŒ‡å‡ºä»¥ä¸‹æƒ³æ³•çš„é—®é¢˜å’Œé£é™©ï¼š

ã€ä¸»é¢˜ã€‘{topic}
ã€å¾…è¾©è®ºæƒ³æ³•ã€‘{idea}

è¯·ä»ä½ çš„ä¸“ä¸šè§’åº¦æä¾›å…·æœ‰å»ºè®¾æ€§çš„è´¨ç–‘ï¼ˆ100å­—ä»¥å†…ï¼‰ï¼š""",
                "model": agent.model_name
            })
        
        # Execute pro and con arguments in parallel
        pro_responses, con_responses = await asyncio.gather(
            self.async_client.get_parallel_completions(pro_prompts),
            self.async_client.get_parallel_completions(con_prompts)
        )
        
        # Format results
        pro_arguments = [
            {"agent": agent.name, "role": agent.role, "argument": response}
            for agent, response in zip(pro_agents, pro_responses)
        ]
        
        con_arguments = [
            {"agent": agent.name, "role": agent.role, "argument": response}
            for agent, response in zip(con_agents, con_responses)
        ]
        
        # Generate synthesis
        synthesis_prompt = f"""è¯·ç»¼åˆä»¥ä¸‹è¾©è®ºå†…å®¹ï¼Œç»™å‡ºå¹³è¡¡çš„ç»“è®ºï¼š

ã€æƒ³æ³•ã€‘{idea}

ã€æ­£æ–¹è§‚ç‚¹ã€‘
{chr(10).join([f"â€¢ {a['agent']}: {a['argument']}" for a in pro_arguments])}

ã€åæ–¹è§‚ç‚¹ã€‘
{chr(10).join([f"â€¢ {a['agent']}: {a['argument']}" for a in con_arguments])}

è¯·ç»™å‡ºï¼š
1. æ ¸å¿ƒå…±è¯†
2. ä¸»è¦åˆ†æ­§
3. å»ºè®®çš„è¡ŒåŠ¨æ–¹å‘"""
        
        synthesis = await self.async_client.get_completion_async(
            system_prompt="ä½ æ˜¯ä¸­ç«‹çš„è¾©è®ºä¸»æŒäººï¼Œæ“…é•¿ç»¼åˆå„æ–¹è§‚ç‚¹ã€‚",
            user_prompt=synthesis_prompt
        )
        
        return {
            "idea": idea,
            "pro_arguments": pro_arguments,
            "con_arguments": con_arguments,
            "synthesis": synthesis
        }
